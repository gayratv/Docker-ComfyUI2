# ğŸŒŸ clip_vision ğŸŒŸ
# Comfy-Org/HunyuanVideo_repackaged
# split_files/clip_vision
https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors
    dir=models/clip_vision
    out=llava_llama3_vision.safetensors

# ğŸŒŸ sigclip_vision_384 ğŸŒŸ
https://huggingface.co/Comfy-Org/sigclip_vision_384/resolve/main/sigclip_vision_patch14_384.safetensors
    dir=models/clip_vision
    out=sigclip_vision_patch14_384.safetensors


# ğŸŒŸ diffusion_models ğŸŒŸ
# split_files/diffusion_models
# 25gb

# v1 â€œconcatâ€
# This first model follows the guiding image less than the other one but might give better movement.
# hunyuan_video_image_to_video_720p_bf16.safetensors
# https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors
#     dir=models/diffusion_models/
#     out=hunyuan_video_image_to_video_720p_bf16.safetensors

# Text to Video
# https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors
#     dir=models/diffusion_models/
#     out=hunyuan_video_t2v_720p_bf16.safetensors

# v2 â€œreplaceâ€
# This second model follows the guiding image very closely but seems to be a bit less dynamic than the first one.
#
# https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors
#     dir=models/diffusion_models/
#     out=hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors

# Kijai/HunyuanVideo_comfy
# FramePackI2V_HY_fp8_e4m3fn.safetensors
https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/FramePackI2V_HY_fp8_e4m3fn.safetensors
    dir=models/diffusion_models/Hyvid
    out=FramePackI2V_HY_fp8_e4m3fn.safetensors

# ğŸŒŸ text_encoders ğŸŒŸ
https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors
    dir=models/clip
    out=clip_l.safetensors
# 16gb
https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp16.safetensors
    dir=models/clip
    out=llava_llama3_fp16.safetensors

# 9gb
# https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors
#    dir=models/clip

# ğŸŒŸ VAE ğŸŒŸ
https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors
    dir=models/vae
    out=hunyuan_video_vae_bf16.safetensors

# And the transformer model itself is either autodownloaded from here:
# https://huggingface.co/lllyasviel/FramePackI2V_HY/tree/main
# to ComfyUI\models\diffusers\lllyasviel\FramePackI2V_HY
# Or from single file, in ComfyUI\models\diffusion_models:
# https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/FramePackI2V_HY_fp8_e4m3fn.safetensors
# https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/FramePackI2V_HY_bf16.safetensors